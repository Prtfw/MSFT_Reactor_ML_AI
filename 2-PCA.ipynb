{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Section 1.2: Dimension reduction and principal component analysis (PCA)\n\nOne of the iron laws of data science is know as the \"[curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)\": as the number of considered features (dimensions) of a space increases, the number of data configurations grows exponentially. Thus the number of observations (data points) needed to account for these configurations must also increase. This fact of life has huge ramifications for the time, computational effort, and memory required. As a result, it is often desirable to reduce the number of dimensions we work with.\n\nThe challenge is determining which features we can remove from our analysis.\n\nNot all features are created equal. Certain features may impede our analysis, or simply not be overly important. We need to identify the features to exclude, but obviously we can only take this so far. At a certain point reducing dimensions will also reduce the accuracy of a classifier and our analysis.\n\n## Our scenario\n\nWe have a collection of foods and their nutrients. We'd like to group them together so we can see similar items. Our traditional categorization of dairy or vegetables aren't descriptive enouth, and can be a little misleading. For example, not all fruits are created equal - an avocado is rather different from a apple. We'd like to group our foods together based on their composition.\n\nAs we begin our analysis, our initial challenge is trying to determine what makes a food item a food item? What components are most important? It's a real struggle to try to determine what components we should focus on."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## PCA in theory\n\nOne way to reduce the number of dimensions we have to work with is by projecting (or converting) our feature space into a lower dimensional space. The reason why we can do this is that in most real-world problems, data points are not spread uniformly across all dimensions. Some features might be near constant, while others are highly correlated, which means that those data points lie close to a lower-dimensional subspace.\n\n\n### An analogy\n\nWe can picture the process of PCA reducing the number of dimensions as we would, well, a picture. A picture is a two-dimensional representation of a three-dimensional world. But the reason a pixel sits where it does on a picture and looks how it looks is because of where it is on the X, Y and Z coordinates which make up the world we see. The picture isn't merely taking a two-dimensional slice, but rather consolidating three dimensions into two **new** dimensions.\n\nThis is especially evident in a picture with a [bokeh](https://en.wikipedia.org/wiki/Bokeh) effect (those pictures where the subject is in focus and everything else is blurry). The photographer has exerted a fair bit of control over how the conversion from 3-D to 2-D is going to take place. The pixels making up our subject will show as sharp, while all other pixels will be fuzzy. Basically, the photographer is drawing our eyes to what's important.\n\nWe can do the same with the dimensions (or columns) in our data, reducing the number and allowing us to focus (pun intended) on the important information.\n\n## PCA in action\n\nPCA behaves much in the same way, by allowing us to focus on what's important, specifically variance. Where are we seeing a spread of data? Where are we seeing change? What's truly impacting the data?\n\nLet's take a look at an example of a three dimensional data space as represented in the image below. The data points are not spread across the entire plane, but are nicely clumped, roughly in an oval. Because the cluster (or, indeed, any cluster) is roughly elliptical, it can be mathematically described by two values: its major (long) axis and its minor (short) axis. These axes form the *principal components* of the cluster. Basically, this is where the magic is happening for our data.\n\n<img align=\"center\" style=\"padding-right:10px;\" src=\"Images/PCA.png\">\n\nWe can **construct a whole new feature space** around this cluster, defined by two *eigenvectors* (the vectors that define the linear transformation to this new feature space), $c_{1}$ and $c_{2}$. Better still, we don't have to consider all of the dimensions of this new space. Intuitively, we can see that most of the points lie on or close to the line that runs through $c_{1}$. So, if we project the cluster down from two dimensions to that single dimension, we capture most of the information about this data sense while simplifying our analysis. This ability to extract most of the information from a dataset by considering only a fraction of its definitive eigenvectors forms the heart of principal component analysis (PCA).\n\n## Import modules and dataset\n\nYou will need to clean and prepare the data in order to conduct PCA on it, so pandas will be essential. You will also need NumPy, a bit of Scikit Learn, and pyplot."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n%matplotlib inline",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The dataset we’ll use here is the same one drawn from the [U.S. Department of Agriculture National Nutrient Database for Standard Reference](https://www.ars.usda.gov/northeast-area/beltsville-md-bhnrc/beltsville-human-nutrition-research-center/nutrient-data-laboratory/docs/usda-national-nutrient-database-for-standard-reference/) that you prepared in Section 1.1. Remember to set the encoding to `latin_1` (for those darn µg)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df = pd.read_csv('Data/USDA-nndb-combined.csv', encoding='latin_1')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can check the number of columns and rows using the `info()` method for the `DataFrame`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise**\n>\n> Can you think of a more concise way to check the number of rows and columns in a `DataFrame`? (***Hint:*** Use one of the [attributes](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) of the `DataFrame`.)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Handle `null` values\n\nBecause this is a real-world dataset, it is a safe bet that it has `null` values in it. We could first check to see if this is true. However, later on in this section, we will have to transform our data using a function that cannot use `NaN` values, so we might as well drop rows containing those values."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise**\n>\n> Drop rows from the `DataFrame` that contain `NaN` values. (If you need help remembering which method to use, see [this page](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html).)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Run the code to drop all null and NaN values\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let’s see how many rows we have left."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Dropping those rows eliminated 76 percent of our data (8989 entries to 2190). An imperfect state of affairs, but we still have enough for our purposes in this section.\n\n> **Key takeaway:** Another solution to removing `null` values is to impute values for them, but this can be tricky. Should we handle missing values as equal to 0? What about a fatty food with `NaN` for `Lipid_Tot_(g)`? We could try taking the averages of values surrounding a `NaN`, but what about foods that are right next to rows containing foods from radically different food groups? It is possible to make justifiable imputations for missing values, but it can be important to involve subject-matter experts (SMEs) in that process."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Split off descriptive columns\n\nOur descriptive columns (such as `FoodGroup` and `Shrt_Desc`) pose challenges for us when it comes time to perform PCA because they are categorical rather than numerical features, so we will split our `DataFrame` in to one containing the descriptive information and one containing the nutritional information."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "desc_df = df.iloc[:, [0, 1, 2]+[i for i in range(50,54)]]\ndesc_df.set_index('NDB_No', inplace=True)\ndesc_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Question**\n>\n> Why was it necessary to structure the `iloc` method call the way we did in the code cell above? What did it accomplish? Why was it necessary set the `desc_df` index to `NDB_No`?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nutr_df = df.iloc[:, :-5] # Remove the last five columns\nnutr_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Question**\n>\n> What did the `iloc` syntax do in the code cell above?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nutr_df = nutr_df.drop(['FoodGroup', 'Shrt_Desc'], axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise**\n>\n> Now set the index of `nutr_df` to use `NDB_No`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise solution**\n>\n> The correct code for students to use here is `nutr_df.set_index('NDB_No', inplace=True)`."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let’s take a look at `nutr_df`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nutr_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Normalize and center the data\n\nOur numeric data comes in a variety of mass units (grams, milligrams, and micrograms) and one energy unit (kilocalories). In order to make an apples-to-apples comparison (pun intended) of the nutritional data, we need to first *normalize* the data and make it more normally distributed (that is, make the distribution of the data look more like a familiar bell curve). PCA needs relatively normalized data to perform its magic.\n\nTo help see why we need to normalize the data, let's look at a histogram of all of the columns."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ax = nutr_df.hist(bins=50, xlabelsize=-1, ylabelsize=-1, figsize=(11,11))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Our first transformation - Box-Cox\n\nNot a bell curve in sight. Worse, a lot of the data is clumped at or around 0. There are a number of ways we can massage the data into a a more useable state.\n\nWe're going to start by trying the [Box-Cox Transformation](https://www.statisticshowto.datasciencecentral.com/box-cox-transformation/) on the data, a popular transformation. It does require a strictly positive input, so we will add 1 to every value in each column."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nutr_df = nutr_df + 1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now for the transformation. The Box-Cox Transformation performs the transformation $y(\\lambda) = \\dfrac{y^{\\lambda}-1}{\\lambda}$ for $\\lambda \\neq 0$ and $y(\\lambda) = log y$ for $\\lambda = 0$ for all values $y$ in a given column. SciPy has a particularly useful `boxcox()` function that can automatically calculate the $\\lambda$ for each column that best normalizes the data in that column. (However, it is does not support `NaN` values; scikit-learn has a comparable `boxcox()` function that is `NaN`-safe, but it is not available on the version of scikit-learn that comes with Azure notebooks.)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from scipy.stats import boxcox\n\nnutr_df_TF = pd.DataFrame(index=nutr_df.index)\nfor col in nutr_df.columns.values:\n    nutr_df_TF['{}_TF'.format(col)] = boxcox(nutr_df.loc[:, col])[0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's now take a look at the `DataFrame` containing the transformed data."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "ax = nutr_df_TF.hist(bins=50, xlabelsize=-1, ylabelsize=-1, figsize=(11,11))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "A few of the columns now look about right, but unfortunately we really didn't see the results we were hoping for, which was bell curves for the bulk of our columns.\n\n### Standard scaling\n\nOur data units were incompatible to begin with, and the transformations haven't given us any real improvement. We can apply a different technique - centering the data around 0; that is, we will again transform the data, this time so that every column has a mean of 0 and a standard deviation of 1. Put another way, we're going to standardize our data around 0. Scikit-learn has a convenient function for this - [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). We're going to call `fit_transform` to perform the conversion. Please note `fit_transform` will return a `numpy` array rather than a `DataFrame`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nutr_df_TF = StandardScaler().fit_transform(nutr_df_TF)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can satisfy your self that the data is now centered by using the `mean()` method on the array."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"mean: \", np.round(nutr_df_TF.mean(), 2))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise**\n>\n> Find the standard deviation for the `nutr_df_TF`. (If you need a hint, you can always use **tab** in Jupyter notebooks)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## PCA in practice\n\nIt is finally time to perform the PCA on our data. (As stated before, even with pretty clean data, a lot of effort has to go into preparing the data for analysis.)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fit = PCA()\npca = fit.fit_transform(nutr_df_TF)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "So, now that we have peformed the PCA on our data, what do we actually have? Remember that PCA is foremost about finding the eigenvectors for our data. We then want to select some subset of those vectors to form the lower-dimensional subspace in which to analyze our data.\n\nLet's take a look at a scatter plot created by the first two components. We can already see some clustering, which we'll be able to better represent when we get towards the end."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.scatter(pca[:,0], pca[:,1], alpha='.1')\nplt.xlabel('C1')\nplt.ylabel('C2')\nplt.title('Collapsed data')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Not all of the eigenvectors are created equal. Just a few of them will account for the majority of the variance in the data. (Put another way, a subspace composed of just a few of the eigenvectors will retain the majority of the information from our data.) We want to focus on those vectors.\n\nTo help us get a sense of how many vectors we should use, consider this scree graph of the variance for the PCA components, which plots the variance explained by the components from greatest to least."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.plot(fit.explained_variance_ratio_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This is where data science can become an art. As a rule of thumb, we want to look for \"elbow\" in the graph, which is the point at which the few components have captured the majority of the variance in the data (after that point, we are only adding complexity to the analysis for increasingly diminishing returns). In this particular case, that appears to be at about five components.\n\nWe can take the cumulative sum of the first five components to see how much variance they capture in total."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(fit.explained_variance_ratio_[:5].sum())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "So our five components capture about 70 percent of the variance. We can see what fewer or additional components would yield by looking at the cumulative variance for all of the components. You can see increasing to 6 components would cover an additional 3 percentage points over the first 5, and there's an ever-diminisioning return."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fit.explained_variance_ratio_.cumsum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can also examine this visually."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.plot(np.cumsum(fit.explained_variance_ratio_))\nplt.title(\"Cumulative Explained Variance Graph\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Ultimately, it is a matter of judgment as to how many components to use, but five vectors (and 70 percent of the variance) will suffice for our purposes in this section.\n\nTo aid further analysis, let's now put those five components into a DataFrame."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pca_df = pd.DataFrame(pca[:, :5], index=desc_df.index)\npca_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Each column represents one of the eigenvectors, and each row is one of the coordinates that defines that vector in five-dimensional space.\n\nWe will want to add the FoodGroup column back in to aid with our interpretation of the data later on. Let's also rename the component-columns $c_{1}$ through $c_{5}$ so that we know what we are looking at."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pca_df = pca_df.join(desc_df)\npca_df.drop(['FoodGroup', 'GmWt_Desc1', 'GmWt_2', 'GmWt_Desc2', 'Refuse_Pct', 'Shrt_Desc'], axis=1, inplace=True)\npca_df.rename(columns={0:'c1', 1:'c2', 2:'c3', 3:'c4', 4:'c5'}, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Don't worry that the FoodGroup column has all `NaN` values: it is not a vector, so it has no vector coordinates.\n\nOne last thing we should demonstrate is that each of the components is mutually perpendicular (or orthogonal in math-speak). One way of expressing that condition is that each component-vector should perfectly correspond with itself and not correlate at all (positively or negatively) with any other vector."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "np.round(pca_df.corr(), 5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Interpreting the results\n\nWhat do our new vectors mean? What is driving our data? To see these results, we will create pandas Series for each of the components, index them by feature, and then sort them in descreasing order (so that a higher number represents a feature that is positively correlated with that vector and negative numbers represent low correlation)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "vects = fit.components_[:5]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "c1 = pd.Series(vects[0], index=nutr_df.columns)\nc1.sort_values(ascending=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Our first component is primarily driven by the amounts of protein, selenium and zinc, while the amounts of vitamin C and sugar have a lessened impact."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "c2 = pd.Series(vects[1], index=nutr_df.columns)\nc2.sort_values(ascending=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Our second component is primarily driven by the amounts of fiber, manganese and folate.\n\nBy using PCA we've been able to distill our data down into fewer, more meaningful dimensions and gain insights we might miss otherwise."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "## Using our PCA results for categorization\n\nOne of the most common uses of PCA is to prepare data for categorization. PCA provides meaningful dimensions which are non-correlated, so our cluster will typically provide better results. This is why we started with PCA to before we attempted to cluster our foods together.\n\nTo perform the categorization, we're going to use [k-means clustering](https://en.wikipedia.org/wiki/K-means_clustering). In a nutshell, k-means will look to identify groups (or clusters) in our data.\n\nWe're going to start by loading the library, creating our model, and then fitting our PCA results into the model. We are arbitrarily using 3 clusters; in [Machine Learning 2](https://github.com/microsoft/Reactors/tree/master/Machine_Learning_2) there is a deeper conversation of k-means and how to determine the best number of clusters. We're using just the first 5 components as we determined it will provide the best return on investment."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.cluster import KMeans\nkmeansmodel = KMeans(n_clusters=3, random_state=13)\nkmeansmodel.fit(pca[:, :5])\nkmeansmodel.labels_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's recreate our `DataFrame` with the PCA values, a new column named **Cluster** which will hold the `label_` or cluster number for each food item, and add back in the text descriptions."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create a new data frame\npca_df = pd.DataFrame(pca[:, :5], index=desc_df.index)\n\n# Add in the cluster number\npca_df['Cluster'] = kmeansmodel.labels_\n\n# Add in the text descriptions\npca_df = pca_df.join(desc_df)\npca_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Exploring our clusters\n\nLet's see the products which wound up in the first cluster."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pca_df.loc[pca_df['Cluster'] == 0]['Shrt_Desc'][:500]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise** Display the first 500 rows for the remaining two clusters"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Display cluster two (remember indexing starts at 0)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Display cluster three (remember indexing starts at 0)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Conclusion\n\nReducing dimensions to allow us to focus on what's important can be a bit of a a challenge. And, frankly, PCA can be a bit confusing at first. PCA's main goal is to provide us with a better signal to noise ratio. It helps us weed out the dimensions which aren't as important, and provide more meaningful insights into the relationships between our data. It's commonly used before categorization algorithms, such as k-means."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}