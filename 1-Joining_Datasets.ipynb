{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Section 1.1: A deep(er) dive into joining and cleaning datasets\n\nAs a general rule, data scientists assume that about 80 percent of the time and effort devoted to a project will be spent preparing data for anlaysis. Because most real-world data is split among multiple existing datasets, this invariably means cleaning and joining diffent datasets together in the way that a data scientist needs for analysis. Thus, mastering these skills is essential for undertaking data science.\n\nThis section makes extensive use of pandas, the principal Python library for data handling and manipulation. Note that this section assumes some familiarity with pandas and basic Python skills.\n\nIn order to provide an experience more like real-world data science, we will use real data taken gathered from the [U.S. Department of Agriculture National Nutrient Database for Standard Reference](https://www.ars.usda.gov/northeast-area/beltsville-md-bhnrc/beltsville-human-nutrition-research-center/nutrient-data-laboratory/docs/usda-national-nutrient-database-for-standard-reference/)."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Reminders about importing, built-in Help, and documentation\n\nThe standard convention in Python-centric data science is to import pandas under the alias `pd`, which is what we will use here:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Because this is such a common convention, it is the way we will use and refer to pandas throughout the rest of this section and this course. You should also adopt this usage in your own code to make it easily readable for other data scientists.\n\nPandas is a big package and there can be a lot to keep track of. Fortunately, IPython (the underlying program that powers this notebook and other like it) gives you the ability to quickly explore the contents of a package like pandas by using its tab-completion feature. If you want to see all of the functions available with pandas, type this:\n\n```ipython\nIn [2]: pd.<TAB>\n\n```\n\nWhen you do so, a drop-down menu will appear next to the `pd`.\n\n> **Exercise**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Place your cursor after the period and press <TAB>:\npd.compat?\n\n# Now select an item using tab-completion and then add a period\n# and use tab-completion to explore *that*.\n# For example, you could try placing pressing <TAB> after:\n# pd.DataFrame.\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As we progress through Section 1.1, don't forget that IPython also provides a tab-completion feature and function documentation with the `?` (question mark) character. If you don't understand something about a function you see in this section, taking a moment to csonult the documentation can help a great deal. You will find this documentation to be a very valuable reference source for your own data science work, both now and in the future. As a reminder, use this code to display the built-in pandas documentation:\n\n```ipython\nIn [4]: pd?\n```\n\n> **Exercise**\n\n> Run this code cell and review the documentation for the pandas DataFrame object. We are going to use it quite a bit."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pd?",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## A brief reminder about Jupyter notebooks\n\nThis course makes extensive use of Jupyter notebooks hosted on Microsoft Azure. Azure-hosted Jupyter notebooks provide an easy way for you to experiment with programming concepts in an interactive fashion that requires no installation of software by students on local computers.\n\nJupyter notebooks are divided into cells. Each cell either contains text written in the Markdown markup language or a space in which to write and execute computer code. Because all the code resides inside code cells, you can run each code cell inline rather than using a separate Python interactive window.\n\n> **Note**: This notebook is designed to have you run code cells one by one, and several code cells contain deliberate errors for demonstration purposes. As a result, if you use the **Cell** > **Run All** command, some code cells past the error won't be run. To resume running the code in each case, use **Cell** > **Run All Below** from the cell after the error."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Loading data\n\n> **Learning goal:** By the end of this subsection, you should be comfortable loading data from files into panda `DataFrame`s and troubleshooting any difficulties that might arise.\n\nBecause pandas `DataFrame`s are two-dimensional data structures, they are inherently similar to flat-file formats such as comma-separated value (CSV) files, the most common import and export format for spreadsheets and databases. Adding to this ease of translation from CSV files to `DataFrame`s, pandas provides a convenient function to load the contents of CSV files into `DataFrame`s (more convenient, in fact, then the native Python [CSV library](https://docs.python.org/3.6/library/csv.html)). Let's get comfortable with [pandas.read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) because we will be using often."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df = pd.read_csv('Data/USDA-nndb.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "After talking up the convenience of pd.read_csv, it might seem strange that we immediately encounter an error. The clue as to what went wrong is in the last line of the error message:\n\n`UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf1 in position 2: invalid continuation byte`\n\nThe data in the CSV file uses a character that the default Unicode Standard ([UTF-8](https://en.wikipedia.org/wiki/UTF-8)) codec reading this file doesn't understand. Remember, this is real-world data and the real world is a messy place.\n\nIt's time to use the pd.read_csv documentation to look for ideas on what to try next.\n\n> **Exercise**\n\n> Use the built-in IPython documentation to on `pd.read_csv.`"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Hint: Refer to the discussion at the start of this section if\n# you forgot the syntax.\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "There are quite a few of parameters for this function. The intuitively named `encoding` parameter accepts `str` input from the list of Python [standard encodings](https://docs.python.org/3.6/library/codecs.html#standard-encodings). We will go with `'latin_1'` here.\n\n**Note:** Although data-science practitioners do develop a familiarity with different encodings they have encountered, selecting the correct encoding can sometimes come down to trial and error, even for professionals!"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df = pd.read_csv('Data/USDA-nndb.csv', encoding='latin_1')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "There was no error message this time, so `'latin_1'` did the trick and we successfully read in the CSV file to the `df` `DataFrame`."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** There is a saying that the difference between data science in academia and the real world is that academia likes to do complex analysis on clean datasets, whereas the real world often does simpler analysis on messier datasets. Troubleshooting difficulties — even ones encountered while merely loading your data — is a large part of successful data science."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Exploring and cleaning the data\n\n> **Learning goal:** By the end of this subsection, you should be comfortable performing simple exploration of your data and performing simple cleaning steps on it to prepare it for later analysis.\n\nData you'll be working with is typically in formats not necessarily designed for human consumption. Fortunately, `DataFrame` offers several tools for exploring the data. Let's explore the data we imported."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can get some aggregated information about the `DataFrame` by using its `info()` method:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Just quickly inspecting the columns from `df` we can see that almost all of the columns have a number of null values. Those missing values are not an issue for us right now, but they will pose a challenge in future sections (but we will deal with them in those sections).\n\nLet's also check to see if this `DataFrame` has any duplicate values in it. Let's start by exploring the `duplicated` method."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.duplicated?",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.duplicated()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You'll notice the results from `duplicated` shows the result on a row by row basis. Not exactly the most efficient way of determining if there's duplicated rows. One quick trick we can use is to call `sum`, which will add the values of the results, where `False` will be **0** and `True` will be **1**. The end result will be we will get a rough sense if there's any duplicated rows."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.duplicated().sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Given the nature of the data source (a government reference database) it makes sense that there are no duplicate entries. For purposes of learning more about cleaning data, let's make a mess so we can see how we can clean it up! Let's start by duplicating data by using the `append()` method."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df = df.append(df, ignore_index=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The `append()` method has basically stacked the `DataFrame` by appending a copy of `df` to the end of the `DataFrame`. (In SQL terms, we performed a [UNION](https://www.w3schools.com/sql/sql_union.asp)). The `ignore_index=True` parameter means that the internal index numbering for the newly doubled `DataFrame` continues seamlessly.\n\nNow let's look directly at how many times individual values in a column (such as `NDB_No`, which is a key) are duplicated. We'll use the `groupby` function to create a \"group\" for each instance of `NDB_No`, and then we'll count each instance."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.groupby('NDB_No')['NDB_No'].count()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Given that we duplicated the original dataset, two duplicates of everything is not unexpected. However, these duplicate values will pose a problem for us later in the section if not dealt with, so let's take care of them now:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df = df.drop_duplicates('NDB_No', keep=\"last\")\ndf.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The `DataFrame` is now half of its previous size, which is what we would expect. However, look at this line in the `df.info()` output:\n\n`Int64Index: 8790 entries, 8790 to 17579`\n\nRemember, counting starts with zero. But while there are only now 8790 entries per column, the indexing for the DataFrame does not run 0 through 8789, as we might have expected. We can see this more directly by looking at the `head` of the redacted `DataFrame`:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Question**\n>\n> Is this behavior of the `drop_duplicates()` method not updating the index values of the `DataFrame` surprising or unexpected for you? Can you explain why this method behaves as it does in this case? If not, study the documentation for this method by using `df.drop_duplicates?` in the code cell below until you're satisfied with your understanding of this behavior."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** Duplicate, `Null`, and `NaN` values can all complicate (if not derail) your analysis. Learning how to identify and remove these problems is a huge part of successfully performing data science."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Splitting the `DataFrame`\n\n> **Learning goal:** By the end of this subsection, you should be comfortable selecting and dropping specific columns from a `DataFrame`.\n\nIt might seem strange to discuss splitting a `DataFrame` in a course section on joining them, but we'll do so here to create the `DataFrame`s that we'll join later on. We take this approach for two reasons:\n\n1. Creating our own `DataFrame`s gives us easy control over the content of the child `DataFrame`s to best demonstrate aspects of joining datasets.\n2. Because we have a baseline, joined `DataFrame` (`df`), it's easy to see how different methods of joining the child `DataFrame`s produce different results.\n\nWe're going to create two child `DataFrame`s, `df1` and `df2`. `df1` will contain the first 35 columns of our data, while `df2` will contain the rest. This will allow us to explore how we can manipulate and manage columns in a dataset, and to simulate a common scenario where some of the data you need is in one location, while the rest is in a different location."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df1 = df.iloc[:,:35]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise**\n\n> Why did we use syntax `df1 = df.iloc[:,:35]` to capture the first 35 columns of `df`? What does the first `:` (colon) in the square brackets do? Experiment with `df3 = df.iloc[:35]` in the code cell below and compare `df3.info()` with `df1.info()` to satisfy yourself as to why we need to use this syntax."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df1.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Let's create df2\n\nWe'll create `df2` in a similar manner to `df1`, but we need to do things a little differently here to ensure that the first columne (`NDB_No`) makes it into `df2`. This is going to serve as the column that's common to both child `DataFrame`s when we join them later in this section.\n\nWe also want to populate `df2` with a different number of rows than `df1`, again simulating real world scenarios. Doing so will make is easier to demonstrate what goes on with some of the join techniques shown below."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df2 = df.iloc[0:2000, [0]+[i for i in range(35,53)]]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Question**\n\n> If you're unsure about why we use `[0] + [i for i in range(35,53)]` in the list comprehension above, review the documentation for the `range()` function using `range?` in the code cell below. You may want to run `[0] + [i for i in range(35,53)]` in the cell below as part of your exploration, and play around with adding (or concatenating) arrays. And remember Python uses zero-based indexing."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can examine `df2` by using the `head()` and `info()` methods."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df2.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df2.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let’s take a look at `df1`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df1.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You'll notice on that both `DataFrame`s have their old indices indexes that they inherited from  `df`. We can fix that by using the `reset_index()` method, but then we run into a problem."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df1 = df1.reset_index()\ndf1.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Our old indexes are still there for `df1`, but now they're in a new column titled `index`. pandas doesn't want to delete data we might need. We can instruct pandas to remove the column, which we know is unnecessary, by using the `drop=True` parameter for the method. (We also need to drop the `index` column we just created in the prior step.)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df1 = df1.drop(['index'], axis=1) #remove the index we created previously\ndf1 = df1.reset_index(drop=True) #reset the index and tell pandas not to create the copy\ndf1.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let's do the same thing to `df2`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df2 = df2.reset_index(drop=True)\ndf2.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "For practice, let's export these `DataFrame`s to CSV files by using the `to_csv()` method. Note that unless we explicitly tell pandas not to, it will also export the index as a column in the CSV file. We will also need to be careful to explicitly encode our CSV to UTF-8."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df1.to_csv('Data/NNDB1.csv', sep=',', encoding='utf-8',index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Export df2 to a CSV file.\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** Although it's not common in the real world to split `DataFrame`s only to re-merge them later, you'll need to drop columns or create new `DataFrame`s that contain only the information you need. With truly large datasets, this is not just a convenience for you analysis, but a necessity for memory and performance!"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Joining `DataFrame`s\n\n> **Learning goal:** By the end of this subsection, you should be comfortable performing left, right, inner, and outer merges on `DataFrame`s.\n\nWe'll examine the  most commonly used `DataFrame` function for joining datasets: `merge()`. But first, let's refresh ourselves on the shapes of our two `DataFrame`s so that the output of our joining makes more sense. This will display the number of rows and columns in each `DataFrame`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df1.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df2.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The type of dataset join that’s most widely used by practicing data scientists is the left join. If you already have some experience with SQL, you know what this refers to. Basically, a left join is a join that takes all of the data from one `DataFrame` (think of it as the left set in a Venn diagram) and merges it with everything that it has in common with another `DataFrame` (the intersection with the right set in the same Venn diagram).\n\nWe do this using the `merge()` function. We also need to specify the type of join we want to perform by using the `how` parameter, as well as the index on which to join the `DataFrames` by using the `on` parameter."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Similar to the SQL:\n# FROM df1 LEFT JOIN df2 ON df1.NBD_No = df2.NBD_No\n\nleft_df = pd.merge(df1, df2, how='left', on='NDB_No')\nleft_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Question**\n\n> Is the shape of the resulting `DataFrame` what you were expecting? Why or why not?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let's compare this to the original `df` `DataFrame`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Question**\n\n> The shapes are the same, but do you expect `df` and `left_df` to be identical? If so, why? If not, what differences do you expect there to be between them?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's check to see what the differences between these `DataFrame`s might be."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "left_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The indexes notwithstanding, the first five rows of both `DataFrame`s are the same. Let's check the last five rows."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.tail()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "left_df.tail()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "There are differences here in the last five rows. Notice that the right-most columns of `left_df` contain have Not a Number (`NaN`) values. This is because the left `DataFrame` was larger than the right `DataFrame`. If you recall, we only took the first 2,000 rows from `df2`."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise**\n\n> A right join is simply the mirror image of a left join in which those entries from the left `DataFrame` that are common with the right `DataFrame` are merged with the right `DataFrame`.\n>\n>Perform a right join of `df1` and `df2` in the code cell below. But before you do that, ask yourself what shape you expect the resulting `DataFrame` to have? Do you expect it to have any `NaN` values?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Hint: the parameter for the right join is how='right'\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Another intuitive and widely used type of join is the inner join. This join simply merges entries that are common to both `DataFrame`s, resulting in a `DataFrame` that has no `NaN` values."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "inner_df = pd.merge(df1, df2, how='inner', on='NDB_No')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Question**\n\n> Before we examine the shape of the resulting `DataFrame`, what do you predict it will be? Why?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "inner_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Question**\n\n> Why are there only 2,000 rows after performing an inner join between `df1` and `df2`?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Did `inner_df` behave as you expected it would? Let's briefly examine it by using the `head()` and `tail()` methods."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "inner_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "inner_df.tail()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The resulting `DataFrame` is essentially the first 2000 rows of the original `df` `DataFrame`."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise**\n\n> An outer join is the union of two `DataFrame`s; anything that is in either `DataFrame` with be in the resultant one. Perform an outer join of `df1` and `df2`. What shape do you expect the resulting `DataFrame` to have? How does it differ from the right join of `df1` and `df2`? What differences would there have to be in the shape or content of either `DataFrame` for the outer join of the two to be different from their right join?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Preparing for coming sections\n\nWe will be using the USDA NNDB dataset in Sections 1.2 and 1.3. However, particularly in Section 1.2, we want to include food group information to go with the food entries to aid with interpreting the result of our data analysis in that section. You will add food group information to this USDA dataset in preparation for these coming sections.\n\nFirst, let's reload our original NNDB dataset so that we have a clean copy."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df = pd.read_csv('Data/USDA-nndb.csv', encoding='latin_1')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let's load in the columns that we want from the older NNDB dataset that includes food groups."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fg_df = pd.read_csv('Data/USDA-nndb-combined.csv', usecols=['NDB_No', 'FoodGroup'])\nfg_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note that `fg_df` does not have the same number of rows as `df`:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fg_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise**\n>\n> We need to combine `df` and `fg_df` using the pandas `merge()` function. As you prepare to do so, keep the following considerations front of mind:\n> 1. Which type of join should you use to capture all of the information in both datasets? (**Hint:** Look at the `head` and `tail` of the resulting `DataFrame` for clues.)\n> 2. In order to put the `FoodGroup` column immediately after the `NDB_No` column, in what order should you enter the two `DataFrame`s into the `merge()` function? (You might need to experiment a couple of times to get the desired order.)\n>\n> Perform the command to join the `df` and `fg_df` in the code cell below."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "combined_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "combined_df.tail()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now save the merged `DataFrame` using the `to_csv()` method."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "combined_df.to_csv('Data/USDA-nndb-merged.csv', \n                   sep=',', \n                   encoding='latin_1', \n                   index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** Because the most interesting insights come from joining different datasets, the pandas `merge()` function is at the heart of most data science projects."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}